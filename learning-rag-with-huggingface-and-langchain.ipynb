{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Setting up environment.","metadata":{}},{"cell_type":"code","source":"!pip install -q uv","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T17:30:53.203001Z","iopub.execute_input":"2025-06-15T17:30:53.203250Z","iopub.status.idle":"2025-06-15T17:30:58.572522Z","shell.execute_reply.started":"2025-06-15T17:30:53.203229Z","shell.execute_reply":"2025-06-15T17:30:58.571563Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.8/17.8 MB\u001b[0m \u001b[31m88.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!uv venv gen-ai","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T17:30:58.574145Z","iopub.execute_input":"2025-06-15T17:30:58.574772Z","iopub.status.idle":"2025-06-15T17:30:58.840815Z","shell.execute_reply.started":"2025-06-15T17:30:58.574729Z","shell.execute_reply":"2025-06-15T17:30:58.840091Z"}},"outputs":[{"name":"stdout","text":"\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1mThe `--system` flag has no effect, a system Python interpreter is always used in `uv venv`\u001b[0m\nUsing CPython 3.11.11 interpreter at: \u001b[36m/usr/bin/python3\u001b[39m\nCreating virtual environment at: \u001b[36mgen-ai\u001b[39m\nActivate with: \u001b[32msource gen-ai/bin/activate\u001b[39m\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!source gen-ai/bin/activate","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T17:30:58.841691Z","iopub.execute_input":"2025-06-15T17:30:58.841973Z","iopub.status.idle":"2025-06-15T17:30:58.965392Z","shell.execute_reply.started":"2025-06-15T17:30:58.841940Z","shell.execute_reply":"2025-06-15T17:30:58.964594Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"!uv pip install -q langchain langchain-huggingface langchain-community langchain unstructured langgraph langchain-core transformers \"torch[cuda]\" \"unstructured[docs]\" ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T17:30:58.966979Z","iopub.execute_input":"2025-06-15T17:30:58.967197Z","iopub.status.idle":"2025-06-15T17:31:18.311333Z","shell.execute_reply.started":"2025-06-15T17:30:58.967176Z","shell.execute_reply":"2025-06-15T17:31:18.310518Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-15T17:31:18.312280Z","iopub.execute_input":"2025-06-15T17:31:18.312545Z","iopub.status.idle":"2025-06-15T17:31:18.593320Z","shell.execute_reply.started":"2025-06-15T17:31:18.312515Z","shell.execute_reply":"2025-06-15T17:31:18.592740Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"# Loading HuggingFace Model using Langchain_HuggingFace","metadata":{}},{"cell_type":"markdown","source":"### Qwen3-0.6B mode","metadata":{}},{"cell_type":"code","source":"from langchain_huggingface.llms import HuggingFacePipeline\nfrom langchain_huggingface import ChatHuggingFace\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, pipeline","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T17:31:18.594101Z","iopub.execute_input":"2025-06-15T17:31:18.594468Z","iopub.status.idle":"2025-06-15T17:31:45.938245Z","shell.execute_reply.started":"2025-06-15T17:31:18.594450Z","shell.execute_reply":"2025-06-15T17:31:45.937488Z"}},"outputs":[{"name":"stderr","text":"2025-06-15 17:31:33.627341: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1750008693.843208      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1750008693.909590      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"llm = HuggingFacePipeline.from_model_id(\n    model_id=\"Qwen/Qwen3-0.6B\",\n    task=\"text-generation\",\n    pipeline_kwargs={\n        \"temperature\": 0.6\n    }\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T01:35:08.194077Z","iopub.execute_input":"2025-06-15T01:35:08.194603Z","iopub.status.idle":"2025-06-15T01:35:20.228070Z","shell.execute_reply.started":"2025-06-15T01:35:08.194581Z","shell.execute_reply":"2025-06-15T01:35:20.227299Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/9.73k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3298584c9b74408b9ce212b21e04fab1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d639b281943d4bb68cc1787e72c9f7c9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9491b654e8704f43a9e1b46a47c3a938"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"88a399d0777642fbafb1253c3c5f830d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/726 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8cfbb3f365c2499e9a62fa11082dd6d2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.50G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d708f7a96506420180375cbcf21e58b7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/239 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"41c2d9992fd4429d8e6dbe29d9ce689b"}},"metadata":{}},{"name":"stderr","text":"Device set to use cuda:0\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"model = ChatHuggingFace(llm=llm)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T01:35:20.228978Z","iopub.execute_input":"2025-06-15T01:35:20.229315Z","iopub.status.idle":"2025-06-15T01:35:20.733847Z","shell.execute_reply.started":"2025-06-15T01:35:20.229275Z","shell.execute_reply":"2025-06-15T01:35:20.733287Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"result = model.invoke(\n    \"explain big bang theory to me like i'm 5?\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T01:36:51.444247Z","iopub.execute_input":"2025-06-15T01:36:51.444856Z","iopub.status.idle":"2025-06-15T01:36:52.193018Z","shell.execute_reply.started":"2025-06-15T01:36:51.444816Z","shell.execute_reply":"2025-06-15T01:36:52.192502Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"result","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T01:36:52.193925Z","iopub.execute_input":"2025-06-15T01:36:52.194176Z","iopub.status.idle":"2025-06-15T01:36:52.198573Z","shell.execute_reply.started":"2025-06-15T01:36:52.194145Z","shell.execute_reply":"2025-06-15T01:36:52.197992Z"}},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"AIMessage(content='<|im_start|>user\\nexplain big bang theory to me like i\\'m 5?<|im_end|>\\n<|im_start|>assistant\\n<think>\\nOkay, so the user wants me to explain \"Big Bang Theory\" to someone like a', additional_kwargs={}, response_metadata={}, id='run--c2e30d3f-adae-49cd-9b7d-c754e47a8312-0')"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"print(result.content)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T01:35:25.450556Z","iopub.execute_input":"2025-06-15T01:35:25.450851Z","iopub.status.idle":"2025-06-15T01:35:25.455528Z","shell.execute_reply.started":"2025-06-15T01:35:25.450824Z","shell.execute_reply":"2025-06-15T01:35:25.454789Z"}},"outputs":[{"name":"stdout","text":"<|im_start|>user\nWhat are large language models?<|im_end|>\n<|im_start|>assistant\n<think>\nOkay, the user asked, \"What are large language models?\" I need to explain this\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"# Inferencing Qwen model using Huggingface","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = \"nvidia/Nemotron-Research-Reasoning-Qwen-1.5B\"\n\n# load the tokenizer and the model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=\"auto\",\n    device_map=\"auto\"\n)\n\n# prepare the model input\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n    {\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True,\n    enable_thinking=True # Switches between thinking and non-thinking modes. Default is True.\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n\n# conduct text completion\ngenerated_ids = model.generate(\n    **model_inputs,\n    max_new_tokens=32768\n)\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() \n\n# parsing thinking content\ntry:\n    # rindex finding 151668 (</think>)\n    index = len(output_ids) - output_ids[::-1].index(151668)\nexcept ValueError:\n    index = 0\n\nthinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\ncontent = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n\nprint(\"thinking content:\", thinking_content)\nprint(\"content:\", content)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T01:40:35.115040Z","iopub.execute_input":"2025-06-15T01:40:35.115749Z","iopub.status.idle":"2025-06-15T01:43:56.130422Z","shell.execute_reply.started":"2025-06-15T01:40:35.115728Z","shell.execute_reply":"2025-06-15T01:43:56.129721Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/9.73k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e69484c056bd4c1887fa2a7026e83ea1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"345f4679cd8a402fa065f02fce126fed"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e7434bbdf44b4ab6af2848b3605a9f8f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"02465ce450b7469bbe2d4c4cba9db3d2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/728 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8c2e2a721ad945afb3b0ad1f791d59c8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/32.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b4e846713290464384889d811ab130ef"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e5ef2f9ea421409199f8b72ae357b8ef"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00005.safetensors:   0%|          | 0.00/3.99G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"160480ae9d0e4abbbdbfd70cfdda04a2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00005-of-00005.safetensors:   0%|          | 0.00/1.24G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fbf0d37b4a7d4d62b8f1f3d4202e7b30"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00004-of-00005.safetensors:   0%|          | 0.00/3.19G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"12c2b25601324e21b9e14979ffbd882a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00005.safetensors:   0%|          | 0.00/4.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1ecc11b44eff45e2bf0611878b5e480c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00005.safetensors:   0%|          | 0.00/3.96G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"304a924f6eb94ca68a9c0dc08aee10bb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"805a31ab52de4fe6a140826b43a38ff5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/239 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5cac8908aa7e499888365a401b9cbe4c"}},"metadata":{}},{"name":"stdout","text":"thinking content: <think>\nOkay, the user is asking for a short introduction to large language models. Let me start by recalling what I know about them. Large language models are AI systems trained on vast amounts of text data, right? They can understand and generate human-like text. But I need to make sure I cover the basics without getting too technical.\n\nFirst, I should mention what they are: AI models trained on massive datasets. Then, their main capabilities—like understanding and generating text. Maybe include examples of tasks they can perform, such as answering questions, writing stories, coding. Also, mention the scale, like billions of parameters. Oh, and the training process involves learning patterns in the data. Should I explain what parameters are? Maybe not in detail since it's a short intro. Also, note that they're used in various applications like chatbots, customer service, content creation. But keep it concise. Let me check if I'm missing anything important. Maybe mention that they're part of the broader field of NLP. Wait, the user might not need that level of detail. Alright, structure the intro to flow from definition, capabilities, training, and applications. Keep each part brief. Avoid jargon where possible. Make sure it's clear and easy to understand. Let me put that together now.\n</think>\ncontent: Large language models (LLMs) are advanced AI systems trained on vast amounts of text data to understand and generate human-like language. They can perform tasks like answering questions, writing stories, coding, and translating languages by learning patterns in the data. These models, often with billions of parameters, are trained to predict the next word in a sequence, enabling them to produce coherent and contextually relevant responses. They are widely used in applications such as chatbots, customer service, content creation, and research, revolutionizing how humans interact with technology.\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"# Mistral_AI using Langchain_HuggingFace","metadata":{}},{"cell_type":"code","source":"from langchain_core.prompts import ChatPromptTemplate\nfrom langchain_huggingface.llms import HuggingFacePipeline\nfrom langchain_huggingface import ChatHuggingFace\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.runnables import RunnableLambda\nfrom langchain_core.messages import HumanMessage\nimport torch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T17:31:54.238200Z","iopub.execute_input":"2025-06-15T17:31:54.239240Z","iopub.status.idle":"2025-06-15T17:31:54.289711Z","shell.execute_reply.started":"2025-06-15T17:31:54.239212Z","shell.execute_reply":"2025-06-15T17:31:54.289213Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"model_id = \"mistralai/Mistral-7B-Instruct-v0.3\"\napi_key = \"\"\n\n# loading tokenizer and model huggingface way\ntokenizer = AutoTokenizer.from_pretrained(model_id, token=api_key)\nmodel = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\", torch_dtype=\"auto\", token=api_key)\npipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=1000)\nllm = HuggingFacePipeline(pipeline=pipe)\nchat_model = ChatHuggingFace(llm=llm)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T17:33:12.055018Z","iopub.execute_input":"2025-06-15T17:33:12.055326Z","iopub.status.idle":"2025-06-15T17:34:06.814101Z","shell.execute_reply.started":"2025-06-15T17:33:12.055303Z","shell.execute_reply":"2025-06-15T17:34:06.813306Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/141k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fa4ccc9b819a4a6ebb4a4919962c69ec"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/587k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5e69916335bc49039aefde15c04bbb8c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"27f3080414844ae892dc7d0f8f334929"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dffb3e3579ca4762a5d366901a69fc67"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/601 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"560500f483f049bb9e601b44634807c0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"df7898464bfb4df5a6b3893e7bd0c645"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8908c7bf0fe14412a0e91acfbc4050fd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"62677167ca124de3a8df3a09da32d24c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00003.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fea4df8c19b74f3cababfb3922dddbef"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00003.safetensors:   0%|          | 0.00/4.55G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ac9f253c4aeb4bdfb83a95b87bc4bf74"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4f93799db643412da5fca20ac93560e9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4eed2a1d62994ec8958275d27557e655"}},"metadata":{}},{"name":"stderr","text":"Device set to use cuda:0\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"from pydantic import BaseModel, Field\nfrom typing import Optional","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T17:34:12.761929Z","iopub.execute_input":"2025-06-15T17:34:12.762199Z","iopub.status.idle":"2025-06-15T17:34:18.510242Z","shell.execute_reply.started":"2025-06-15T17:34:12.762178Z","shell.execute_reply":"2025-06-15T17:34:18.509476Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# Output class structure\nclass ModelOutput(BaseModel):\n    answer: str\n\ndef parse_mistral_output(raw_output: str) -> ModelOutput:\n    special_token = \"[/INST]\"\n\n    try:\n        # index where special token is found\n        start_index = raw_output.rindex(special_token) + len(special_token)\n\n        extracted_text = raw_output[start_index:]\n\n        clean_extracted_text = extracted_text.strip()\n\n        return ModelOutput(answer=clean_extracted_text)\n\n    except ValueError:\n        print(f\"Warning: {special_token} not found in text!\")\n        return ModelOutput(answer=raw_output.strip())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T17:45:00.640040Z","iopub.execute_input":"2025-06-15T17:45:00.640877Z","iopub.status.idle":"2025-06-15T17:45:00.646540Z","shell.execute_reply.started":"2025-06-15T17:45:00.640824Z","shell.execute_reply":"2025-06-15T17:45:00.645814Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"prompt_template = ChatPromptTemplate([\n    (\"system\", \"You are a Theoretical Physicist with expertise in string theory\"),\n    (\"human\", \"{question}\")\n])\n\nmodel_output_parser = StrOutputParser()\nmistral_output_parser = RunnableLambda(parse_mistral_output)\nchat_chain = prompt_template | chat_model | model_output_parser | mistral_output_parser","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T17:45:01.789189Z","iopub.execute_input":"2025-06-15T17:45:01.789464Z","iopub.status.idle":"2025-06-15T17:45:01.794691Z","shell.execute_reply.started":"2025-06-15T17:45:01.789445Z","shell.execute_reply":"2025-06-15T17:45:01.793930Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"question = \"What is String Theory? Give me a detailed example.\"\nresponse = chat_chain.invoke({\"question\": question})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T17:45:31.477793Z","iopub.execute_input":"2025-06-15T17:45:31.478068Z","iopub.status.idle":"2025-06-15T17:46:00.953327Z","shell.execute_reply.started":"2025-06-15T17:45:31.478051Z","shell.execute_reply":"2025-06-15T17:46:00.952794Z"}},"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"print(response)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T17:46:14.070078Z","iopub.execute_input":"2025-06-15T17:46:14.070532Z","iopub.status.idle":"2025-06-15T17:46:14.074415Z","shell.execute_reply.started":"2025-06-15T17:46:14.070510Z","shell.execute_reply":"2025-06-15T17:46:14.073895Z"}},"outputs":[{"name":"stdout","text":"answer=\"String Theory is a theoretical framework in which the point-like particles of particle physics are replaced by one-dimensional objects called strings. It is a candidate for a unified theory of all fundamental forces and forms of matter, including gravity, electromagnetism, and the nuclear forces.\\n\\nIn String Theory, strings can vibrate at different frequencies, and these vibrations correspond to different fundamental particles. For example, a string vibrating at a certain frequency could correspond to a photon (the particle of light), while a string vibrating at a different frequency could correspond to an electron.\\n\\nLet's consider a simple example to illustrate this idea. Imagine a guitar string. When you pluck the string, it vibrates at a certain frequency, producing a note. If you pluck the string harder or shorter, the frequency changes, producing a different note. In the same way, a string in String Theory can vibrate at different frequencies, each frequency corresponding to a different particle.\\n\\nOne of the most intriguing aspects of String Theory is the concept of extra dimensions. In our everyday experience, we live in a world with three dimensions of space (length, width, and height) and one dimension of time. However, String Theory suggests that there may be up to 10 or more dimensions in the universe. These extra dimensions are compactified, meaning they are curled up so small that they are not observable at our scale.\\n\\nFor example, imagine a rubber sheet with a tiny, curled-up straw on it. From our perspective, the straw appears to be a point, even though it has one dimension of length. In the same way, the extra dimensions in String Theory are curled up so small that they are not observable at our scale.\\n\\nString Theory is still a work in progress, and many of its predictions have not been confirmed by experiments. However, it is one of the most promising candidates for a unified theory of the universe, and it continues to be an active area of research in theoretical physics.\"\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"print(response.answer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T17:46:32.091868Z","iopub.execute_input":"2025-06-15T17:46:32.092152Z","iopub.status.idle":"2025-06-15T17:46:32.096507Z","shell.execute_reply.started":"2025-06-15T17:46:32.092133Z","shell.execute_reply":"2025-06-15T17:46:32.095815Z"}},"outputs":[{"name":"stdout","text":"String Theory is a theoretical framework in which the point-like particles of particle physics are replaced by one-dimensional objects called strings. It is a candidate for a unified theory of all fundamental forces and forms of matter, including gravity, electromagnetism, and the nuclear forces.\n\nIn String Theory, strings can vibrate at different frequencies, and these vibrations correspond to different fundamental particles. For example, a string vibrating at a certain frequency could correspond to a photon (the particle of light), while a string vibrating at a different frequency could correspond to an electron.\n\nLet's consider a simple example to illustrate this idea. Imagine a guitar string. When you pluck the string, it vibrates at a certain frequency, producing a note. If you pluck the string harder or shorter, the frequency changes, producing a different note. In the same way, a string in String Theory can vibrate at different frequencies, each frequency corresponding to a different particle.\n\nOne of the most intriguing aspects of String Theory is the concept of extra dimensions. In our everyday experience, we live in a world with three dimensions of space (length, width, and height) and one dimension of time. However, String Theory suggests that there may be up to 10 or more dimensions in the universe. These extra dimensions are compactified, meaning they are curled up so small that they are not observable at our scale.\n\nFor example, imagine a rubber sheet with a tiny, curled-up straw on it. From our perspective, the straw appears to be a point, even though it has one dimension of length. In the same way, the extra dimensions in String Theory are curled up so small that they are not observable at our scale.\n\nString Theory is still a work in progress, and many of its predictions have not been confirmed by experiments. However, it is one of the most promising candidates for a unified theory of the universe, and it continues to be an active area of research in theoretical physics.\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"prompt_template2 = ChatPromptTemplate([\n    (\"system\", \"You are a Senior Data Scientist. You have a working experience of 15+ years.\"),\n    (\"user\", \"{question}\"),\n])\n\nchat_chain2 = prompt_template2 | chat_model | model_output_parser | mistral_output_parser","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T17:47:12.032044Z","iopub.execute_input":"2025-06-15T17:47:12.032316Z","iopub.status.idle":"2025-06-15T17:47:12.036568Z","shell.execute_reply.started":"2025-06-15T17:47:12.032295Z","shell.execute_reply":"2025-06-15T17:47:12.035855Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"question2 = \"list all data cleaning and handling techniques?\"\nresponse2 = chat_chain2.invoke({\"question\": question2})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T17:47:16.403696Z","iopub.execute_input":"2025-06-15T17:47:16.404492Z","iopub.status.idle":"2025-06-15T17:48:12.088688Z","shell.execute_reply.started":"2025-06-15T17:47:16.404445Z","shell.execute_reply":"2025-06-15T17:48:12.088114Z"}},"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"print(response2.answer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T17:48:31.409361Z","iopub.execute_input":"2025-06-15T17:48:31.409649Z","iopub.status.idle":"2025-06-15T17:48:31.413918Z","shell.execute_reply.started":"2025-06-15T17:48:31.409629Z","shell.execute_reply":"2025-06-15T17:48:31.412941Z"}},"outputs":[{"name":"stdout","text":"As a Senior Data Scientist with over 15 years of experience, I have employed various data cleaning and handling techniques throughout my career. Here is a list of some common techniques:\n\n1. **Handling Missing Values**:\n   - Removing rows or columns with missing values (depending on the data and the analysis)\n   - Filling missing values with mean, median, mode, or other statistical measures\n   - Using advanced techniques like multiple imputation, regression imputation, or k-nearest neighbors imputation\n\n2. **Data Normalization and Standardization**:\n   - Min-Max Scaling: Scaling data between 0 and 1\n   - Z-Score Standardization: Scaling data to have a mean of 0 and standard deviation of 1\n   - Log Transformation: Transforming data to reduce skewness and improve model performance\n\n3. **Outlier Detection and Handling**:\n   - Identifying outliers using statistical methods (e.g., Z-score, IQR) or machine learning algorithms\n   - Removing outliers or replacing them with mean, median, or other statistical measures\n   - Using robust statistical methods that are less sensitive to outliers\n\n4. **Data Aggregation**:\n   - Combining multiple data points into a single data point (e.g., aggregating daily data into monthly data)\n   - Grouping data based on certain criteria (e.g., grouping customers by age range)\n\n5. **Data Merging and Joining**:\n   - Inner join: Combining rows from two tables where the join condition is true in both tables\n   - Outer join: Combining rows from two tables even if the join condition is not true in one or both tables\n   - Left join, right join, and full join: Variations of outer join that return specific types of joined rows\n\n6. **Data Munging**:\n   - Renaming columns\n   - Changing data types (e.g., converting strings to numbers)\n   - Splitting columns (e.g., splitting a single column containing first and last names into separate columns)\n   - Combining columns (e.g., concatenating first and last names into a single column)\n\n7. **Data Sampling**:\n   - Random sampling: Selecting a random subset of data for analysis\n   - Stratified sampling: Ensuring that each stratum (subgroup) in the data is proportionally represented in the sample\n   - Clustered sampling: Selecting clusters of data and then selecting a random sample from each cluster\n\n8. **Data Balancing**:\n   - Oversampling the minority class to balance the dataset\n   - Undersampling the majority class to balance the dataset\n   - SMOTE (Synthetic Minority Over-sampling Technique): Generating synthetic minority class samples to balance the dataset\n\n9. **Handling Imbalanced Classes**:\n   - Using cost-sensitive learning algorithms that give more weight to the minority class\n   - Using ensemble methods like Random Forest or AdaBoost that can handle imbalanced data\n   - Using class weighting during training to give more weight to the minority class\n\n10. **Data Privacy and Security**:\n    - Anonymizing data by removing or encrypting sensitive information\n    - Using techniques like k-anonymity or l-diversity to protect individual privacy\n    - Implementing data access controls to ensure data security\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Parsing Mistral Output","metadata":{}},{"cell_type":"code","source":"import pydantic\nfrom typing import Optional\n\n# 1. Define the Pydantic Model for the parsed output\n# This class defines the structure of our final, clean output.\n# We want to extract the model's answer into a single string field.\nclass ModelOutput(pydantic.BaseModel):\n    \"\"\"Pydantic model to hold the parsed answer from the LLM.\"\"\"\n    answer: str\n    \n    # You could add other fields here if you wanted to extract more structured data.\n    # For example:\n    # techniques: Optional[list[str]] = None\n\n# 2. Create a parsing function\ndef parse_mistral_output(raw_output: str) -> ModelOutput:\n    \"\"\"\n    Parses the raw string output from the Mistral model to extract the final answer.\n\n    Args:\n        raw_output: The complete string returned by the language model.\n\n    Returns:\n        A Pydantic ModelOutput object containing the clean answer.\n    \"\"\"\n    # The model's actual answer comes after the [/INST] tag.\n    # We find the position of this tag and take the substring that follows it.\n    inst_marker = \"[/INST]\"\n    \n    try:\n        # Find the index where the marker ends\n        answer_start_index = raw_output.rindex(inst_marker) + len(inst_marker)\n        \n        # Extract the substring after the marker\n        extracted_text = raw_output[answer_start_index:]\n        \n        # Clean up any leading/trailing whitespace\n        clean_text = extracted_text.strip()\n        \n        # Create and return the Pydantic model instance\n        return ModelOutput(answer=clean_text)\n\n    except ValueError:\n        # This will happen if the \"[/INST]\" marker is not in the output.\n        # In this case, we can assume the whole output is the answer,\n        # or handle it as an error. For robustness, we'll treat it as the answer.\n        print(\"Warning: '[/INST]' marker not found. Treating entire output as the answer.\")\n        return ModelOutput(answer=raw_output.strip())\n\n\n# --- Example Usage ---\n\n# This is the raw output you provided from the Hugging Face model\nsample_output = \"\"\"\n<s>[INST] You are a Senior Data Scientist. You have a working experience of 15+ years.\n\nlist all data cleaning and handling techniques?[/INST] As a Senior Data Scientist with over 15 years of experience, I have employed various data cleaning and handling techniques throughout my career. Here is a list of some common techniques:\n\n1. **Handling Missing Values**:\n   - Removing rows or columns with missing values (depending on the data and the analysis)\n   - Filling missing values with mean, median, mode, or other statistical measures\n   - Using advanced techniques like multiple imputation, regression imputation, or k-nearest neighbors imputation\n\n2. **Data Normalization and Standardization**:\n   - Min-Max Scaling: Scaling data between 0 and 1\n   - Z-Score Standardization: Scaling data to have a mean of 0 and standard deviation of 1\n   - Log Transformation: Transforming data to reduce skewness and improve model performance\n\n3. **Outlier Detection and Handling**:\n   - Identifying outliers using statistical methods (e.g., Z-score, IQR) or machine learning algorithms\n   - Removing outliers or replacing them with mean, median, or other statistical measures\n   - Using robust statistical methods that are less sensitive to outliers\n\"\"\"\n\n# Parse the output\nparsed_data = parse_mistral_output(sample_output)\n\n# Now you have a structured object, and you can access the clean answer easily\nprint(\"--- Parsed LLM Answer ---\")\nprint(parsed_data.answer)\n\n# You can also easily convert the Pydantic object to a dictionary if needed\nprint(\"\\n--- As Dictionary ---\")\nprint(parsed_data.model_dump())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T08:24:12.123862Z","iopub.execute_input":"2025-06-15T08:24:12.124134Z","iopub.status.idle":"2025-06-15T08:24:12.132467Z","shell.execute_reply.started":"2025-06-15T08:24:12.124117Z","shell.execute_reply":"2025-06-15T08:24:12.131710Z"}},"outputs":[{"name":"stdout","text":"--- Parsed LLM Answer ---\nAs a Senior Data Scientist with over 15 years of experience, I have employed various data cleaning and handling techniques throughout my career. Here is a list of some common techniques:\n\n1. **Handling Missing Values**:\n   - Removing rows or columns with missing values (depending on the data and the analysis)\n   - Filling missing values with mean, median, mode, or other statistical measures\n   - Using advanced techniques like multiple imputation, regression imputation, or k-nearest neighbors imputation\n\n2. **Data Normalization and Standardization**:\n   - Min-Max Scaling: Scaling data between 0 and 1\n   - Z-Score Standardization: Scaling data to have a mean of 0 and standard deviation of 1\n   - Log Transformation: Transforming data to reduce skewness and improve model performance\n\n3. **Outlier Detection and Handling**:\n   - Identifying outliers using statistical methods (e.g., Z-score, IQR) or machine learning algorithms\n   - Removing outliers or replacing them with mean, median, or other statistical measures\n   - Using robust statistical methods that are less sensitive to outliers\n\n--- As Dictionary ---\n{'answer': 'As a Senior Data Scientist with over 15 years of experience, I have employed various data cleaning and handling techniques throughout my career. Here is a list of some common techniques:\\n\\n1. **Handling Missing Values**:\\n   - Removing rows or columns with missing values (depending on the data and the analysis)\\n   - Filling missing values with mean, median, mode, or other statistical measures\\n   - Using advanced techniques like multiple imputation, regression imputation, or k-nearest neighbors imputation\\n\\n2. **Data Normalization and Standardization**:\\n   - Min-Max Scaling: Scaling data between 0 and 1\\n   - Z-Score Standardization: Scaling data to have a mean of 0 and standard deviation of 1\\n   - Log Transformation: Transforming data to reduce skewness and improve model performance\\n\\n3. **Outlier Detection and Handling**:\\n   - Identifying outliers using statistical methods (e.g., Z-score, IQR) or machine learning algorithms\\n   - Removing outliers or replacing them with mean, median, or other statistical measures\\n   - Using robust statistical methods that are less sensitive to outliers'}\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}